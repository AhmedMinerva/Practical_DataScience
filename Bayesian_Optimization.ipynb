{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Activity 2: Bayesian Optimization - Group 3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmedMinerva/Practical_DataScience/blob/master/Bayesian_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6f1nw0WSL-V"
      },
      "source": [
        "# Activity 2: Bayesian Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMnXFrBgSPmN"
      },
      "source": [
        "## Part 1: Bayesian Optimization on King's County Houses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8PlG8yBSZJ1"
      },
      "source": [
        "You will find, in the following cells, that the code to (a) create an XGBoost model to predict the prices of houses of the dataset you used for pre-class work and (b) an implementation of the BayesianOptimization algorithm to optimize hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTwm2280Sudw"
      },
      "source": [
        "### Run the cells below, and check to see if you understand each step!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxtHHuzHNhq6",
        "outputId": "df0d4a09-6055-4b3b-8f46-11abb5e14239",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install bayesian-optimization #we have to instal the bayesian-optimization package"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/7a/fd8059a3881d3ab37ac8f72f56b73937a14e8bb14a9733e68cc8b17dbe3c/bayesian-optimization-1.2.0.tar.gz\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (0.17.0)\n",
            "Building wheels for collected packages: bayesian-optimization\n",
            "  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.2.0-cp36-none-any.whl size=11685 sha256=1b1c431a38888a956c40e84ad67e8836d9208abc29cd3b8553888d3cc51b865a\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/56/ae/e0e3c1fc1954dc3ec712e2df547235ed072b448094d8f94aec\n",
            "Successfully built bayesian-optimization\n",
            "Installing collected packages: bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzcUTCaNCpzu"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "data = pd.read_csv(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vRaQcPHF6GaPB5bHKF1Q6ndb4l2Gv4CIXmFqSTeZi1c7OqKuYM9HHHoBIotsxQiM7Yjr9K0Qb6lhnDI/pub?output=csv\") #Importing the data\n",
        "data = data.drop(labels=[\"id\", \"date\"], axis=1) #Dropping these columns\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.loc[:, data.columns != 'price'], data['price'], test_size=0.25, random_state=42) #Splitting test and train!\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSnKJ2mxDyXK",
        "outputId": "ea179b0e-91d7-467f-8f90-1647094a69a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data_dmatrix = xgb.DMatrix(data=X_train, label=y_train) #converting our test and train to a data matrix, do you know why??\n",
        "params = {\"objective\":'reg:squarederror', \"colsample_bytree\":0.2, \"learning_rate\":0.08, \"max_depth\":4, \"alpha\":16} #defining our parameters through a dictionary\n",
        "xg_m = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=70) #training the model!\n",
        "\n",
        "data_dmatrix_test = xgb.DMatrix(data=X_test)#preparing the test data for prediction\n",
        "preds = xg_m.predict(data_dmatrix_test) #predicting the test data\n",
        "\n",
        "np.sqrt(mean_squared_error(preds, y_test)) #How well did we do?"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "168732.47527618211"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-WmAYq0D6Yz",
        "outputId": "8aeb3faa-c6ce-46b9-a9f9-0bc92b17c69c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#for the cross validation process, I wrote a function that takes in certian parameters\n",
        "#and outputs the minimum rmse after cross validation!\n",
        "def fcv(max_depth, gamma, min_child_weight, subsample, colsample_bytree, learning_rate, num_boost_round):\n",
        "  params = {\"objective\":'reg:squarederror', \"max_depth\":int(max_depth), 'gamma':gamma, 'min_child_weight':min_child_weight, 'subsample':subsample, \"colsample_bytree\":colsample_bytree, \"learning_rate\":learning_rate}\n",
        "  cv_results=xgb.cv(dtrain=data_dmatrix, params=params, nfold=10, num_boost_round=int(num_boost_round), early_stopping_rounds=10, metrics='rmse', as_pandas=True)\n",
        "  return -cv_results['test-rmse-mean'].min() #any idea why I used a negative sign? Hint: it matters for the Bayesian Optimization function\n",
        "\n",
        "fcv(4, 3, 0.5, 0.2, 0.5, 0.5, 70) #random settings to see if it works"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-156913.3515625"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8D2vZ9U1E9iz",
        "outputId": "33fcb729-8763-4452-b1fe-387da70dcf4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dict_cv = {\n",
        "          'max_depth': (2, 12),\n",
        "          'gamma': (0.001, 10.0),\n",
        "          'min_child_weight': (0, 20),\n",
        "          'subsample': (0.4, 1.0),\n",
        "          'colsample_bytree': (0.4, 1.0),\n",
        "          'learning_rate': (0.1, 1.0),\n",
        "          'num_boost_round' :(30, 100)\n",
        "          }\n",
        "#Creating a dictionary with the ranges for each parameter in a tuple! Note that the \n",
        "#Dictionary's keys HAVE to match the keys for the cross validation (fcv) function\n",
        "\n",
        "\n",
        "XGB_BO = BayesianOptimization(fcv, dict_cv) #Creating the optimizer\n",
        "XGB_BO.maximize(init_points=10, n_iter=30, acq='ei', xi=0.0) #Running optimization!"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|   iter    |  target   | colsam... |   gamma   | learni... | max_depth | min_ch... | num_bo... | subsample |\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "| \u001b[0m 1       \u001b[0m | \u001b[0m-1.526e+0\u001b[0m | \u001b[0m 0.7778  \u001b[0m | \u001b[0m 8.058   \u001b[0m | \u001b[0m 0.8737  \u001b[0m | \u001b[0m 6.738   \u001b[0m | \u001b[0m 10.9    \u001b[0m | \u001b[0m 63.36   \u001b[0m | \u001b[0m 0.8741  \u001b[0m |\n",
            "| \u001b[95m 2       \u001b[0m | \u001b[95m-1.465e+0\u001b[0m | \u001b[95m 0.809   \u001b[0m | \u001b[95m 3.321   \u001b[0m | \u001b[95m 0.8349  \u001b[0m | \u001b[95m 3.41    \u001b[0m | \u001b[95m 12.53   \u001b[0m | \u001b[95m 65.71   \u001b[0m | \u001b[95m 0.7268  \u001b[0m |\n",
            "| \u001b[95m 3       \u001b[0m | \u001b[95m-1.327e+0\u001b[0m | \u001b[95m 0.5519  \u001b[0m | \u001b[95m 7.0     \u001b[0m | \u001b[95m 0.4686  \u001b[0m | \u001b[95m 8.362   \u001b[0m | \u001b[95m 9.9     \u001b[0m | \u001b[95m 44.76   \u001b[0m | \u001b[95m 0.9664  \u001b[0m |\n",
            "| \u001b[0m 4       \u001b[0m | \u001b[0m-1.471e+0\u001b[0m | \u001b[0m 0.9208  \u001b[0m | \u001b[0m 5.058   \u001b[0m | \u001b[0m 0.2331  \u001b[0m | \u001b[0m 2.538   \u001b[0m | \u001b[0m 16.03   \u001b[0m | \u001b[0m 57.08   \u001b[0m | \u001b[0m 0.6146  \u001b[0m |\n",
            "| \u001b[95m 5       \u001b[0m | \u001b[95m-1.19e+05\u001b[0m | \u001b[95m 0.8274  \u001b[0m | \u001b[95m 3.049   \u001b[0m | \u001b[95m 0.1509  \u001b[0m | \u001b[95m 5.811   \u001b[0m | \u001b[95m 1.633   \u001b[0m | \u001b[95m 82.93   \u001b[0m | \u001b[95m 0.668   \u001b[0m |\n",
            "| \u001b[0m 6       \u001b[0m | \u001b[0m-1.234e+0\u001b[0m | \u001b[0m 0.8891  \u001b[0m | \u001b[0m 4.871   \u001b[0m | \u001b[0m 0.2508  \u001b[0m | \u001b[0m 8.767   \u001b[0m | \u001b[0m 6.022   \u001b[0m | \u001b[0m 97.16   \u001b[0m | \u001b[0m 0.7134  \u001b[0m |\n",
            "| \u001b[0m 7       \u001b[0m | \u001b[0m-1.242e+0\u001b[0m | \u001b[0m 0.6381  \u001b[0m | \u001b[0m 8.834   \u001b[0m | \u001b[0m 0.2981  \u001b[0m | \u001b[0m 8.586   \u001b[0m | \u001b[0m 0.7406  \u001b[0m | \u001b[0m 31.77   \u001b[0m | \u001b[0m 0.8523  \u001b[0m |\n",
            "| \u001b[0m 8       \u001b[0m | \u001b[0m-1.637e+0\u001b[0m | \u001b[0m 0.7171  \u001b[0m | \u001b[0m 7.898   \u001b[0m | \u001b[0m 0.8291  \u001b[0m | \u001b[0m 3.776   \u001b[0m | \u001b[0m 12.68   \u001b[0m | \u001b[0m 32.96   \u001b[0m | \u001b[0m 0.4421  \u001b[0m |\n",
            "| \u001b[0m 9       \u001b[0m | \u001b[0m-1.43e+05\u001b[0m | \u001b[0m 0.5265  \u001b[0m | \u001b[0m 2.819   \u001b[0m | \u001b[0m 0.644   \u001b[0m | \u001b[0m 10.06   \u001b[0m | \u001b[0m 5.867   \u001b[0m | \u001b[0m 94.27   \u001b[0m | \u001b[0m 0.9354  \u001b[0m |\n",
            "| \u001b[0m 10      \u001b[0m | \u001b[0m-1.414e+0\u001b[0m | \u001b[0m 0.4422  \u001b[0m | \u001b[0m 4.631   \u001b[0m | \u001b[0m 0.595   \u001b[0m | \u001b[0m 8.641   \u001b[0m | \u001b[0m 11.55   \u001b[0m | \u001b[0m 82.34   \u001b[0m | \u001b[0m 0.8496  \u001b[0m |\n",
            "| \u001b[0m 11      \u001b[0m | \u001b[0m-1.309e+0\u001b[0m | \u001b[0m 0.5336  \u001b[0m | \u001b[0m 9.432   \u001b[0m | \u001b[0m 0.2891  \u001b[0m | \u001b[0m 4.274   \u001b[0m | \u001b[0m 14.75   \u001b[0m | \u001b[0m 52.79   \u001b[0m | \u001b[0m 0.7907  \u001b[0m |\n",
            "| \u001b[0m 12      \u001b[0m | \u001b[0m-1.256e+0\u001b[0m | \u001b[0m 0.9134  \u001b[0m | \u001b[0m 2.221   \u001b[0m | \u001b[0m 0.1737  \u001b[0m | \u001b[0m 5.888   \u001b[0m | \u001b[0m 15.45   \u001b[0m | \u001b[0m 89.69   \u001b[0m | \u001b[0m 0.8031  \u001b[0m |\n",
            "| \u001b[0m 13      \u001b[0m | \u001b[0m-1.482e+0\u001b[0m | \u001b[0m 0.4667  \u001b[0m | \u001b[0m 3.918   \u001b[0m | \u001b[0m 0.6467  \u001b[0m | \u001b[0m 6.629   \u001b[0m | \u001b[0m 2.599   \u001b[0m | \u001b[0m 58.08   \u001b[0m | \u001b[0m 0.5122  \u001b[0m |\n",
            "| \u001b[0m 14      \u001b[0m | \u001b[0m-1.766e+0\u001b[0m | \u001b[0m 0.5097  \u001b[0m | \u001b[0m 3.074   \u001b[0m | \u001b[0m 0.918   \u001b[0m | \u001b[0m 11.81   \u001b[0m | \u001b[0m 6.915   \u001b[0m | \u001b[0m 94.58   \u001b[0m | \u001b[0m 0.4989  \u001b[0m |\n",
            "| \u001b[0m 15      \u001b[0m | \u001b[0m-1.304e+0\u001b[0m | \u001b[0m 0.7752  \u001b[0m | \u001b[0m 5.41    \u001b[0m | \u001b[0m 0.2428  \u001b[0m | \u001b[0m 3.972   \u001b[0m | \u001b[0m 8.559   \u001b[0m | \u001b[0m 82.15   \u001b[0m | \u001b[0m 0.626   \u001b[0m |\n",
            "| \u001b[0m 16      \u001b[0m | \u001b[0m-1.27e+05\u001b[0m | \u001b[0m 0.7355  \u001b[0m | \u001b[0m 3.297   \u001b[0m | \u001b[0m 0.3218  \u001b[0m | \u001b[0m 11.81   \u001b[0m | \u001b[0m 6.233   \u001b[0m | \u001b[0m 92.57   \u001b[0m | \u001b[0m 0.8572  \u001b[0m |\n",
            "| \u001b[0m 17      \u001b[0m | \u001b[0m-1.508e+0\u001b[0m | \u001b[0m 0.8367  \u001b[0m | \u001b[0m 0.3123  \u001b[0m | \u001b[0m 0.6316  \u001b[0m | \u001b[0m 7.734   \u001b[0m | \u001b[0m 17.38   \u001b[0m | \u001b[0m 70.9    \u001b[0m | \u001b[0m 0.4115  \u001b[0m |\n",
            "| \u001b[0m 18      \u001b[0m | \u001b[0m-1.703e+0\u001b[0m | \u001b[0m 0.4761  \u001b[0m | \u001b[0m 5.792   \u001b[0m | \u001b[0m 0.9186  \u001b[0m | \u001b[0m 11.08   \u001b[0m | \u001b[0m 11.9    \u001b[0m | \u001b[0m 96.32   \u001b[0m | \u001b[0m 0.6788  \u001b[0m |\n",
            "| \u001b[0m 19      \u001b[0m | \u001b[0m-1.313e+0\u001b[0m | \u001b[0m 0.425   \u001b[0m | \u001b[0m 9.918   \u001b[0m | \u001b[0m 0.4086  \u001b[0m | \u001b[0m 5.789   \u001b[0m | \u001b[0m 12.2    \u001b[0m | \u001b[0m 65.01   \u001b[0m | \u001b[0m 0.7238  \u001b[0m |\n",
            "| \u001b[0m 20      \u001b[0m | \u001b[0m-1.665e+0\u001b[0m | \u001b[0m 0.6391  \u001b[0m | \u001b[0m 7.868   \u001b[0m | \u001b[0m 0.9549  \u001b[0m | \u001b[0m 10.73   \u001b[0m | \u001b[0m 16.58   \u001b[0m | \u001b[0m 73.69   \u001b[0m | \u001b[0m 0.6003  \u001b[0m |\n",
            "| \u001b[0m 21      \u001b[0m | \u001b[0m-1.322e+0\u001b[0m | \u001b[0m 0.5511  \u001b[0m | \u001b[0m 0.8519  \u001b[0m | \u001b[0m 0.4065  \u001b[0m | \u001b[0m 8.466   \u001b[0m | \u001b[0m 11.02   \u001b[0m | \u001b[0m 77.56   \u001b[0m | \u001b[0m 0.6723  \u001b[0m |\n",
            "| \u001b[0m 22      \u001b[0m | \u001b[0m-1.275e+0\u001b[0m | \u001b[0m 0.5526  \u001b[0m | \u001b[0m 3.28    \u001b[0m | \u001b[0m 0.2145  \u001b[0m | \u001b[0m 9.169   \u001b[0m | \u001b[0m 17.49   \u001b[0m | \u001b[0m 89.24   \u001b[0m | \u001b[0m 0.631   \u001b[0m |\n",
            "| \u001b[0m 23      \u001b[0m | \u001b[0m-1.318e+0\u001b[0m | \u001b[0m 0.8126  \u001b[0m | \u001b[0m 3.979   \u001b[0m | \u001b[0m 0.5494  \u001b[0m | \u001b[0m 7.924   \u001b[0m | \u001b[0m 11.14   \u001b[0m | \u001b[0m 62.47   \u001b[0m | \u001b[0m 0.8825  \u001b[0m |\n",
            "| \u001b[0m 24      \u001b[0m | \u001b[0m-1.426e+0\u001b[0m | \u001b[0m 0.6607  \u001b[0m | \u001b[0m 3.287   \u001b[0m | \u001b[0m 0.5866  \u001b[0m | \u001b[0m 2.08    \u001b[0m | \u001b[0m 16.48   \u001b[0m | \u001b[0m 90.06   \u001b[0m | \u001b[0m 0.606   \u001b[0m |\n",
            "| \u001b[0m 25      \u001b[0m | \u001b[0m-1.281e+0\u001b[0m | \u001b[0m 0.84    \u001b[0m | \u001b[0m 6.527   \u001b[0m | \u001b[0m 0.2872  \u001b[0m | \u001b[0m 5.772   \u001b[0m | \u001b[0m 5.959   \u001b[0m | \u001b[0m 61.79   \u001b[0m | \u001b[0m 0.5281  \u001b[0m |\n",
            "| \u001b[0m 26      \u001b[0m | \u001b[0m-1.565e+0\u001b[0m | \u001b[0m 0.9539  \u001b[0m | \u001b[0m 4.785   \u001b[0m | \u001b[0m 0.9546  \u001b[0m | \u001b[0m 4.432   \u001b[0m | \u001b[0m 11.46   \u001b[0m | \u001b[0m 48.13   \u001b[0m | \u001b[0m 0.6268  \u001b[0m |\n",
            "| \u001b[0m 27      \u001b[0m | \u001b[0m-1.413e+0\u001b[0m | \u001b[0m 0.4423  \u001b[0m | \u001b[0m 0.2921  \u001b[0m | \u001b[0m 0.5248  \u001b[0m | \u001b[0m 2.375   \u001b[0m | \u001b[0m 0.1404  \u001b[0m | \u001b[0m 55.22   \u001b[0m | \u001b[0m 0.9485  \u001b[0m |\n",
            "| \u001b[0m 28      \u001b[0m | \u001b[0m-1.345e+0\u001b[0m | \u001b[0m 0.5676  \u001b[0m | \u001b[0m 7.234   \u001b[0m | \u001b[0m 0.3868  \u001b[0m | \u001b[0m 3.43    \u001b[0m | \u001b[0m 16.25   \u001b[0m | \u001b[0m 87.72   \u001b[0m | \u001b[0m 0.59    \u001b[0m |\n",
            "| \u001b[0m 29      \u001b[0m | \u001b[0m-1.587e+0\u001b[0m | \u001b[0m 0.8333  \u001b[0m | \u001b[0m 8.2     \u001b[0m | \u001b[0m 0.8902  \u001b[0m | \u001b[0m 2.01    \u001b[0m | \u001b[0m 13.73   \u001b[0m | \u001b[0m 45.71   \u001b[0m | \u001b[0m 0.5992  \u001b[0m |\n",
            "| \u001b[0m 30      \u001b[0m | \u001b[0m-1.525e+0\u001b[0m | \u001b[0m 0.6577  \u001b[0m | \u001b[0m 7.632   \u001b[0m | \u001b[0m 0.8692  \u001b[0m | \u001b[0m 5.68    \u001b[0m | \u001b[0m 8.943   \u001b[0m | \u001b[0m 66.99   \u001b[0m | \u001b[0m 0.8523  \u001b[0m |\n",
            "| \u001b[0m 31      \u001b[0m | \u001b[0m-1.382e+0\u001b[0m | \u001b[0m 0.4788  \u001b[0m | \u001b[0m 2.628   \u001b[0m | \u001b[0m 0.6763  \u001b[0m | \u001b[0m 6.655   \u001b[0m | \u001b[0m 0.7875  \u001b[0m | \u001b[0m 80.46   \u001b[0m | \u001b[0m 0.8483  \u001b[0m |\n",
            "| \u001b[0m 32      \u001b[0m | \u001b[0m-1.533e+0\u001b[0m | \u001b[0m 0.7387  \u001b[0m | \u001b[0m 4.391   \u001b[0m | \u001b[0m 0.6061  \u001b[0m | \u001b[0m 8.676   \u001b[0m | \u001b[0m 9.807   \u001b[0m | \u001b[0m 66.83   \u001b[0m | \u001b[0m 0.4111  \u001b[0m |\n",
            "| \u001b[0m 33      \u001b[0m | \u001b[0m-1.202e+0\u001b[0m | \u001b[0m 0.8183  \u001b[0m | \u001b[0m 2.778   \u001b[0m | \u001b[0m 0.1752  \u001b[0m | \u001b[0m 10.16   \u001b[0m | \u001b[0m 0.5723  \u001b[0m | \u001b[0m 62.1    \u001b[0m | \u001b[0m 0.6411  \u001b[0m |\n",
            "| \u001b[0m 34      \u001b[0m | \u001b[0m-1.447e+0\u001b[0m | \u001b[0m 0.8559  \u001b[0m | \u001b[0m 0.01695 \u001b[0m | \u001b[0m 0.1878  \u001b[0m | \u001b[0m 3.804   \u001b[0m | \u001b[0m 9.524   \u001b[0m | \u001b[0m 32.54   \u001b[0m | \u001b[0m 0.8122  \u001b[0m |\n",
            "| \u001b[0m 35      \u001b[0m | \u001b[0m-1.6e+05 \u001b[0m | \u001b[0m 0.4541  \u001b[0m | \u001b[0m 1.604   \u001b[0m | \u001b[0m 0.8583  \u001b[0m | \u001b[0m 11.43   \u001b[0m | \u001b[0m 3.135   \u001b[0m | \u001b[0m 85.13   \u001b[0m | \u001b[0m 0.8201  \u001b[0m |\n",
            "| \u001b[0m 36      \u001b[0m | \u001b[0m-1.276e+0\u001b[0m | \u001b[0m 0.573   \u001b[0m | \u001b[0m 6.239   \u001b[0m | \u001b[0m 0.378   \u001b[0m | \u001b[0m 6.597   \u001b[0m | \u001b[0m 7.623   \u001b[0m | \u001b[0m 69.65   \u001b[0m | \u001b[0m 0.6879  \u001b[0m |\n",
            "| \u001b[0m 37      \u001b[0m | \u001b[0m-1.499e+0\u001b[0m | \u001b[0m 0.7896  \u001b[0m | \u001b[0m 1.054   \u001b[0m | \u001b[0m 0.8082  \u001b[0m | \u001b[0m 9.384   \u001b[0m | \u001b[0m 5.619   \u001b[0m | \u001b[0m 31.97   \u001b[0m | \u001b[0m 0.7984  \u001b[0m |\n",
            "| \u001b[0m 38      \u001b[0m | \u001b[0m-1.351e+0\u001b[0m | \u001b[0m 0.881   \u001b[0m | \u001b[0m 4.996   \u001b[0m | \u001b[0m 0.575   \u001b[0m | \u001b[0m 7.943   \u001b[0m | \u001b[0m 6.796   \u001b[0m | \u001b[0m 48.97   \u001b[0m | \u001b[0m 0.9685  \u001b[0m |\n",
            "| \u001b[0m 39      \u001b[0m | \u001b[0m-1.657e+0\u001b[0m | \u001b[0m 0.6832  \u001b[0m | \u001b[0m 1.248   \u001b[0m | \u001b[0m 0.9382  \u001b[0m | \u001b[0m 8.216   \u001b[0m | \u001b[0m 4.262   \u001b[0m | \u001b[0m 98.75   \u001b[0m | \u001b[0m 0.628   \u001b[0m |\n",
            "| \u001b[0m 40      \u001b[0m | \u001b[0m-1.228e+0\u001b[0m | \u001b[0m 0.7485  \u001b[0m | \u001b[0m 4.921   \u001b[0m | \u001b[0m 0.1399  \u001b[0m | \u001b[0m 9.986   \u001b[0m | \u001b[0m 5.453   \u001b[0m | \u001b[0m 31.82   \u001b[0m | \u001b[0m 0.9392  \u001b[0m |\n",
            "=============================================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYofJ839X2Vk"
      },
      "source": [
        "## Part 2: Building Bayesian Optimization Yourself!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GGgI9DmX77Z"
      },
      "source": [
        "Now, it is time for you to try your hand at Bayesian Optimization! To do this, we will work with a new dataset, but that has the exact same premise: we have around 84 variables that predict the sale price of a house in Ames, Iowa.\n",
        "\n",
        "Your task is, again, to use XGBoost to predict sale prices. You will notice that the data processing and model training steps are already done for you below: what you have to do next is to implement the cross validation and bayesian optimization steps of the code above!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkEjvPREXRba"
      },
      "source": [
        "data2 = pd.read_csv(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vTp6iMy3iSfMS-3BzqX5wtu4AFSlZZVn8QFNeScSrJmsGLC29tIqarJ3I5ODb-SusrCNZ0hoNnHTqp-/pub?output=csv\")\n",
        "data2.head()\n",
        "\n",
        "#delete columns with many missing data\n",
        "data2.drop(['PoolQC','MiscFeature','Alley','Fence','FireplaceQu','LotFrontage'], axis = 1,inplace=True)\n",
        "\n",
        "#Drop rows with missing data \n",
        "data2.dropna(inplace=True)\n",
        "data2.shape\n",
        "\n",
        "data2 = pd.get_dummies(data2)\n",
        "X_train, X_test, y_train, y_test = train_test_split(data2.loc[:, data2.columns != 'SalePrice'], data2['SalePrice'], test_size=0.25, random_state=42)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmPZp3OSZHGo",
        "outputId": "5d0b6b1b-bcd3-43c2-dc2c-7a239f12b9b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data_dmatrix2 = xgb.DMatrix(data=X_train, label=y_train) #converting our test and train to a data matrix, do you know why??\n",
        "params = {\"objective\":'reg:squarederror', \"colsample_bytree\":0.2, \"learning_rate\":0.08, \"max_depth\":4, \"alpha\":16} #defining our parameters through a dictionary\n",
        "xg_m2 = xgb.train(params=params, dtrain=data_dmatrix2, num_boost_round=70) #training the model!\n",
        "\n",
        "data_dmatrix_test2 = xgb.DMatrix(data=X_test, label=y_test)#preparing the test data for prediction\n",
        "preds = xg_m2.predict(data_dmatrix_test2) #predicting the test data\n",
        "\n",
        "print(np.sqrt(mean_squared_error(preds, y_test)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32389.117783905178\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLdNoTE_MMym"
      },
      "source": [
        "#Define here your cross validation function!!\n",
        "def fcv(max_depth, gamma, min_child_weight, subsample, colsample_bytree, learning_rate, num_boost_round):\n",
        "  params = {\"objective\":'reg:squarederror', \"max_depth\":int(max_depth), 'gamma':gamma, 'min_child_weight':min_child_weight, 'subsample':subsample, \"colsample_bytree\":colsample_bytree, \"learning_rate\":learning_rate}\n",
        "  cv_results=xgb.cv(dtrain=data_dmatrix, params=params, nfold=10, num_boost_round=int(num_boost_round), early_stopping_rounds=10, metrics='rmse', as_pandas=True)\n",
        "  return -cv_results['test-rmse-mean'].min()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMAV1YRWMaV6",
        "outputId": "b91ffc0c-63f3-49e6-e232-31851501bdf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Now, create a dictionary for the boundaries we should search within, and call\n",
        "#the bayesian optimization function!\n",
        "dict_cv = {\n",
        "          'max_depth': (2, 12),\n",
        "          'gamma': (0.001, 10.0),\n",
        "          'min_child_weight': (0, 20),\n",
        "          'subsample': (0.4, 1.0),\n",
        "          'colsample_bytree': (0.4, 1.0),\n",
        "          'learning_rate': (0.1, 1.0),\n",
        "          'num_boost_round' :(30, 100)\n",
        "          }\n",
        "\n",
        "\n",
        "\n",
        "XGB_BO = BayesianOptimization(fcv, dict_cv) #Creating the optimizer\n",
        "XGB_BO.maximize(init_points=10, n_iter=20, acq='ei', xi=0.0) #Running optimization!"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|   iter    |  target   | colsam... |   gamma   | learni... | max_depth | min_ch... | num_bo... | subsample |\n",
            "-------------------------------------------------------------------------------------------------------------\n",
            "| \u001b[0m 1       \u001b[0m | \u001b[0m-1.408e+0\u001b[0m | \u001b[0m 0.438   \u001b[0m | \u001b[0m 7.126   \u001b[0m | \u001b[0m 0.5951  \u001b[0m | \u001b[0m 5.619   \u001b[0m | \u001b[0m 19.13   \u001b[0m | \u001b[0m 65.65   \u001b[0m | \u001b[0m 0.5187  \u001b[0m |\n",
            "| \u001b[95m 2       \u001b[0m | \u001b[95m-1.305e+0\u001b[0m | \u001b[95m 0.6899  \u001b[0m | \u001b[95m 5.592   \u001b[0m | \u001b[95m 0.4893  \u001b[0m | \u001b[95m 6.146   \u001b[0m | \u001b[95m 10.84   \u001b[0m | \u001b[95m 58.25   \u001b[0m | \u001b[95m 0.9691  \u001b[0m |\n",
            "| \u001b[0m 3       \u001b[0m | \u001b[0m-1.387e+0\u001b[0m | \u001b[0m 0.7276  \u001b[0m | \u001b[0m 9.468   \u001b[0m | \u001b[0m 0.5308  \u001b[0m | \u001b[0m 9.927   \u001b[0m | \u001b[0m 12.61   \u001b[0m | \u001b[0m 42.92   \u001b[0m | \u001b[0m 0.6859  \u001b[0m |\n",
            "| \u001b[0m 4       \u001b[0m | \u001b[0m-1.694e+0\u001b[0m | \u001b[0m 0.5077  \u001b[0m | \u001b[0m 2.746   \u001b[0m | \u001b[0m 0.9959  \u001b[0m | \u001b[0m 6.519   \u001b[0m | \u001b[0m 19.42   \u001b[0m | \u001b[0m 58.82   \u001b[0m | \u001b[0m 0.577   \u001b[0m |\n",
            "| \u001b[0m 5       \u001b[0m | \u001b[0m-1.425e+0\u001b[0m | \u001b[0m 0.9183  \u001b[0m | \u001b[0m 5.637   \u001b[0m | \u001b[0m 0.8141  \u001b[0m | \u001b[0m 3.129   \u001b[0m | \u001b[0m 3.194   \u001b[0m | \u001b[0m 55.69   \u001b[0m | \u001b[0m 0.9508  \u001b[0m |\n",
            "| \u001b[0m 6       \u001b[0m | \u001b[0m-1.39e+05\u001b[0m | \u001b[0m 0.9858  \u001b[0m | \u001b[0m 3.873   \u001b[0m | \u001b[0m 0.3535  \u001b[0m | \u001b[0m 3.176   \u001b[0m | \u001b[0m 10.19   \u001b[0m | \u001b[0m 40.09   \u001b[0m | \u001b[0m 0.8102  \u001b[0m |\n",
            "| \u001b[0m 7       \u001b[0m | \u001b[0m-1.646e+0\u001b[0m | \u001b[0m 0.7962  \u001b[0m | \u001b[0m 2.994   \u001b[0m | \u001b[0m 0.9474  \u001b[0m | \u001b[0m 7.85    \u001b[0m | \u001b[0m 11.48   \u001b[0m | \u001b[0m 46.56   \u001b[0m | \u001b[0m 0.6143  \u001b[0m |\n",
            "| \u001b[0m 8       \u001b[0m | \u001b[0m-1.54e+05\u001b[0m | \u001b[0m 0.8571  \u001b[0m | \u001b[0m 2.475   \u001b[0m | \u001b[0m 0.7952  \u001b[0m | \u001b[0m 4.828   \u001b[0m | \u001b[0m 11.03   \u001b[0m | \u001b[0m 42.04   \u001b[0m | \u001b[0m 0.6022  \u001b[0m |\n",
            "| \u001b[0m 9       \u001b[0m | \u001b[0m-1.343e+0\u001b[0m | \u001b[0m 0.7226  \u001b[0m | \u001b[0m 3.459   \u001b[0m | \u001b[0m 0.4307  \u001b[0m | \u001b[0m 4.505   \u001b[0m | \u001b[0m 3.488   \u001b[0m | \u001b[0m 38.65   \u001b[0m | \u001b[0m 0.6098  \u001b[0m |\n",
            "| \u001b[95m 10      \u001b[0m | \u001b[95m-1.293e+0\u001b[0m | \u001b[95m 0.6766  \u001b[0m | \u001b[95m 4.5     \u001b[0m | \u001b[95m 0.1515  \u001b[0m | \u001b[95m 6.342   \u001b[0m | \u001b[95m 11.94   \u001b[0m | \u001b[95m 31.88   \u001b[0m | \u001b[95m 0.6456  \u001b[0m |\n",
            "| \u001b[0m 11      \u001b[0m | \u001b[0m-1.804e+0\u001b[0m | \u001b[0m 0.4361  \u001b[0m | \u001b[0m 7.903   \u001b[0m | \u001b[0m 0.9627  \u001b[0m | \u001b[0m 8.02    \u001b[0m | \u001b[0m 13.74   \u001b[0m | \u001b[0m 48.51   \u001b[0m | \u001b[0m 0.4214  \u001b[0m |\n",
            "| \u001b[95m 12      \u001b[0m | \u001b[95m-1.192e+0\u001b[0m | \u001b[95m 0.6427  \u001b[0m | \u001b[95m 1.876   \u001b[0m | \u001b[95m 0.1447  \u001b[0m | \u001b[95m 11.04   \u001b[0m | \u001b[95m 2.926   \u001b[0m | \u001b[95m 92.85   \u001b[0m | \u001b[95m 0.8323  \u001b[0m |\n",
            "| \u001b[0m 13      \u001b[0m | \u001b[0m-1.315e+0\u001b[0m | \u001b[0m 0.537   \u001b[0m | \u001b[0m 6.028   \u001b[0m | \u001b[0m 0.3659  \u001b[0m | \u001b[0m 10.01   \u001b[0m | \u001b[0m 14.44   \u001b[0m | \u001b[0m 89.81   \u001b[0m | \u001b[0m 0.7947  \u001b[0m |\n",
            "| \u001b[0m 14      \u001b[0m | \u001b[0m-1.456e+0\u001b[0m | \u001b[0m 0.5601  \u001b[0m | \u001b[0m 8.448   \u001b[0m | \u001b[0m 0.5106  \u001b[0m | \u001b[0m 10.6    \u001b[0m | \u001b[0m 5.002   \u001b[0m | \u001b[0m 41.68   \u001b[0m | \u001b[0m 0.4446  \u001b[0m |\n",
            "| \u001b[0m 15      \u001b[0m | \u001b[0m-1.48e+05\u001b[0m | \u001b[0m 0.6202  \u001b[0m | \u001b[0m 2.175   \u001b[0m | \u001b[0m 0.9987  \u001b[0m | \u001b[0m 4.096   \u001b[0m | \u001b[0m 0.2722  \u001b[0m | \u001b[0m 86.09   \u001b[0m | \u001b[0m 0.9739  \u001b[0m |\n",
            "| \u001b[0m 16      \u001b[0m | \u001b[0m-1.422e+0\u001b[0m | \u001b[0m 0.5347  \u001b[0m | \u001b[0m 0.333   \u001b[0m | \u001b[0m 0.8046  \u001b[0m | \u001b[0m 2.895   \u001b[0m | \u001b[0m 5.321   \u001b[0m | \u001b[0m 60.46   \u001b[0m | \u001b[0m 0.9038  \u001b[0m |\n",
            "| \u001b[0m 17      \u001b[0m | \u001b[0m-1.645e+0\u001b[0m | \u001b[0m 0.4614  \u001b[0m | \u001b[0m 2.771   \u001b[0m | \u001b[0m 0.9073  \u001b[0m | \u001b[0m 11.79   \u001b[0m | \u001b[0m 19.58   \u001b[0m | \u001b[0m 43.76   \u001b[0m | \u001b[0m 0.7745  \u001b[0m |\n",
            "| \u001b[0m 18      \u001b[0m | \u001b[0m-1.754e+0\u001b[0m | \u001b[0m 0.4556  \u001b[0m | \u001b[0m 2.192   \u001b[0m | \u001b[0m 0.8338  \u001b[0m | \u001b[0m 9.423   \u001b[0m | \u001b[0m 4.744   \u001b[0m | \u001b[0m 76.73   \u001b[0m | \u001b[0m 0.4154  \u001b[0m |\n",
            "| \u001b[0m 19      \u001b[0m | \u001b[0m-1.518e+0\u001b[0m | \u001b[0m 0.9006  \u001b[0m | \u001b[0m 3.088   \u001b[0m | \u001b[0m 0.234   \u001b[0m | \u001b[0m 2.673   \u001b[0m | \u001b[0m 13.06   \u001b[0m | \u001b[0m 44.27   \u001b[0m | \u001b[0m 0.4596  \u001b[0m |\n",
            "| \u001b[0m 20      \u001b[0m | \u001b[0m-1.37e+05\u001b[0m | \u001b[0m 0.7155  \u001b[0m | \u001b[0m 6.463   \u001b[0m | \u001b[0m 0.5972  \u001b[0m | \u001b[0m 7.882   \u001b[0m | \u001b[0m 11.35   \u001b[0m | \u001b[0m 89.06   \u001b[0m | \u001b[0m 0.9113  \u001b[0m |\n",
            "| \u001b[0m 21      \u001b[0m | \u001b[0m-1.32e+05\u001b[0m | \u001b[0m 0.7551  \u001b[0m | \u001b[0m 9.088   \u001b[0m | \u001b[0m 0.6053  \u001b[0m | \u001b[0m 3.769   \u001b[0m | \u001b[0m 14.12   \u001b[0m | \u001b[0m 92.06   \u001b[0m | \u001b[0m 0.8881  \u001b[0m |\n",
            "| \u001b[0m 22      \u001b[0m | \u001b[0m-1.199e+0\u001b[0m | \u001b[0m 0.6506  \u001b[0m | \u001b[0m 0.7697  \u001b[0m | \u001b[0m 0.3018  \u001b[0m | \u001b[0m 4.989   \u001b[0m | \u001b[0m 3.893   \u001b[0m | \u001b[0m 86.24   \u001b[0m | \u001b[0m 0.9171  \u001b[0m |\n",
            "| \u001b[0m 23      \u001b[0m | \u001b[0m-1.523e+0\u001b[0m | \u001b[0m 0.9862  \u001b[0m | \u001b[0m 2.803   \u001b[0m | \u001b[0m 0.822   \u001b[0m | \u001b[0m 11.82   \u001b[0m | \u001b[0m 4.719   \u001b[0m | \u001b[0m 58.01   \u001b[0m | \u001b[0m 0.708   \u001b[0m |\n",
            "| \u001b[0m 24      \u001b[0m | \u001b[0m-1.255e+0\u001b[0m | \u001b[0m 0.636   \u001b[0m | \u001b[0m 7.57    \u001b[0m | \u001b[0m 0.386   \u001b[0m | \u001b[0m 6.887   \u001b[0m | \u001b[0m 5.714   \u001b[0m | \u001b[0m 80.32   \u001b[0m | \u001b[0m 0.9468  \u001b[0m |\n",
            "| \u001b[0m 25      \u001b[0m | \u001b[0m-1.55e+05\u001b[0m | \u001b[0m 0.9263  \u001b[0m | \u001b[0m 0.4583  \u001b[0m | \u001b[0m 0.6513  \u001b[0m | \u001b[0m 2.46    \u001b[0m | \u001b[0m 12.16   \u001b[0m | \u001b[0m 31.19   \u001b[0m | \u001b[0m 0.5355  \u001b[0m |\n",
            "| \u001b[0m 26      \u001b[0m | \u001b[0m-1.273e+0\u001b[0m | \u001b[0m 0.8238  \u001b[0m | \u001b[0m 2.349   \u001b[0m | \u001b[0m 0.2946  \u001b[0m | \u001b[0m 6.667   \u001b[0m | \u001b[0m 19.93   \u001b[0m | \u001b[0m 61.09   \u001b[0m | \u001b[0m 0.8222  \u001b[0m |\n",
            "| \u001b[0m 27      \u001b[0m | \u001b[0m-1.265e+0\u001b[0m | \u001b[0m 0.8787  \u001b[0m | \u001b[0m 8.439   \u001b[0m | \u001b[0m 0.2161  \u001b[0m | \u001b[0m 8.678   \u001b[0m | \u001b[0m 13.26   \u001b[0m | \u001b[0m 92.41   \u001b[0m | \u001b[0m 0.7168  \u001b[0m |\n",
            "| \u001b[0m 28      \u001b[0m | \u001b[0m-1.449e+0\u001b[0m | \u001b[0m 0.6931  \u001b[0m | \u001b[0m 4.861   \u001b[0m | \u001b[0m 0.9221  \u001b[0m | \u001b[0m 3.678   \u001b[0m | \u001b[0m 17.59   \u001b[0m | \u001b[0m 74.77   \u001b[0m | \u001b[0m 0.9532  \u001b[0m |\n",
            "| \u001b[0m 29      \u001b[0m | \u001b[0m-1.345e+0\u001b[0m | \u001b[0m 0.6108  \u001b[0m | \u001b[0m 4.502   \u001b[0m | \u001b[0m 0.4844  \u001b[0m | \u001b[0m 5.88    \u001b[0m | \u001b[0m 19.68   \u001b[0m | \u001b[0m 35.9    \u001b[0m | \u001b[0m 0.798   \u001b[0m |\n",
            "| \u001b[0m 30      \u001b[0m | \u001b[0m-1.524e+0\u001b[0m | \u001b[0m 0.7582  \u001b[0m | \u001b[0m 1.417   \u001b[0m | \u001b[0m 0.7931  \u001b[0m | \u001b[0m 5.353   \u001b[0m | \u001b[0m 5.901   \u001b[0m | \u001b[0m 54.88   \u001b[0m | \u001b[0m 0.7284  \u001b[0m |\n",
            "=============================================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}